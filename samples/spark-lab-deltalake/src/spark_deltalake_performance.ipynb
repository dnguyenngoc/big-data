{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64b5aed-a520-43fb-95ac-816d8c570689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import pycountry\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e11114d-3731-4c47-b486-0fe2c8cdddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added new country 'Japan' with common names: ['Haruto', 'Yui', 'Haruka', 'Yuki', 'Hiroto', 'Sakura', 'Sora', 'Aoi', 'Riku', 'Miyu']\n",
      "Added new country 'Italy' with common names: ['Alessio', 'Sofia', 'Giuseppe', 'Aria', 'Matteo', 'Giulia', 'Marco', 'Emma', 'Lorenzo', 'Aurora']\n",
      "Added new country 'Australia' with common names: ['Oliver', 'Charlotte', 'William', 'Ava', 'Noah', 'Mia', 'Jack', 'Amelia', 'Leo', 'Isla']\n",
      "Added new country 'France' with common names: ['Lucas', 'Louise', 'Hugo', 'Manon', 'Gabriel', 'Chloé', 'Raphaël', 'Emma', 'Jules', 'Léa']\n",
      "Added new country 'South Korea' with common names: ['Min-Joon', 'Ji-Yeon', 'Hyeon-Woo', 'Seo-Yeon', 'Dae-Hyun', 'Ha-Neul', 'Sang-Min', 'Su-Ji', 'Hyun-Jae', 'Yu-Jin']\n",
      "Added new country 'Mexico' with common names: ['Carlos', 'Sofía', 'Alejandro', 'Valentina', 'Javier', 'Camila', 'Miguel', 'Andrea', 'Diego', 'Isabella']\n",
      "Added new country 'Canada' with common names: ['Ethan', 'Olivia', 'Liam', 'Emma', 'Noah', 'Sophia', 'Lucas', 'Ava', 'Benjamin', 'Amelia']\n",
      "Added new country 'Egypt' with common names: ['Ahmed', 'Nour', 'Omar', 'Hana', 'Mohamed', 'Sara', 'Ali', 'Yasmin', 'Khaled', 'Mariam']\n",
      "Added new country 'Nigeria' with common names: ['Chinedu', 'Adebisi', 'Obinna', 'Chidinma', 'Olufemi', 'Chiamaka', 'Emeka', 'Adesua', 'Uche', 'Ngozi']\n",
      "Added new country 'Thailand' with common names: ['Narin', 'Saranya', 'Phirun', 'Kanya', 'Tawin', 'Arunee', 'Nawin', 'Bundit', 'Sopa', 'Chai']\n",
      "Added new country 'Spain' with common names: ['Alejandro', 'Sofía', 'David', 'Marta', 'Pablo', 'Lucía', 'Carlos', 'Laura', 'Javier', 'Alba']\n",
      "Added new country 'Greece' with common names: ['Giannis', 'Maria', 'Nikos', 'Eleni', 'Dimitris', 'Katerina', 'Yannis', 'Despina', 'Panagiotis', 'Anastasia']\n",
      "Added new country 'Argentina' with common names: ['Mateo', 'Sofía', 'Valentín', 'Camila', 'Benjamín', 'Valentina', 'Santiago', 'Emma', 'Lucas', 'Mia']\n",
      "Added new country 'Sweden' with common names: ['Elias', 'Astrid', 'Oscar', 'Emilia', 'Liam', 'Alice', 'Noah', 'Olivia', 'Viktor', 'Ella']\n",
      "Added new country 'Switzerland' with common names: ['Liam', 'Emma', 'Noah', 'Mia', 'Elias', 'Sofia', 'Lukas', 'Lina', 'Gabriel', 'Laura']\n",
      "Added new country 'South Africa' with common names: ['Lwazi', 'Nompumelelo', 'Kagiso', 'Lerato', 'Thabo', 'Buhle', 'Sipho', 'Nokuthula', 'Thabiso', 'Nomvula']\n",
      "Added new country 'Indonesia' with common names: ['Rizky', 'Ayu', 'Wahyu', 'Rani', 'Adi', 'Sari', 'Budi', 'Fitri', 'Dian', 'Hadi']\n",
      "Added new country 'Philippines' with common names: ['Juan', 'Maria', 'Javier', 'Ana', 'Gabriel', 'Sophia', 'Emmanuel', 'Luisa', 'Miguel', 'Isabella']\n",
      "Added new country 'Poland' with common names: ['Jakub', 'Zuzanna', 'Szymon', 'Julia', 'Filip', 'Maja', 'Kacper', 'Lena', 'Bartosz', 'Oliwia']\n",
      "Added new country 'Iran' with common names: ['Amir', 'Sara', 'Mohammad', 'Yasmin', 'Ali', 'Hoda', 'Hassan', 'Mahsa', 'Saeed', 'Leila']\n",
      "Added new country 'Germany' with common names: ['Lukas', 'Hannah', 'Tim', 'Anna', 'Felix', 'Lea', 'Jan', 'Laura', 'Maximilian', 'Sophie']\n",
      "Added new country 'Brazil' with common names: ['Carlos', 'Ana', 'Pedro', 'Camila', 'Lucas', 'Isabella', 'Mateus', 'Juliana', 'Rafael', 'Laura']\n",
      "Added new country 'Russia' with common names: ['Ivan', 'Olga', 'Vladimir', 'Anastasia', 'Dmitry', 'Svetlana', 'Nikolai', 'Tatiana', 'Roman', 'Maria']\n",
      "Added new country 'China' with common names: ['Wei', 'Ling', 'Ming', 'Li', 'Yan', 'Chen', 'Jing', 'Tao', 'Mei', 'Jun']\n",
      "Added new country 'India' with common names: ['Ravi', 'Priya', 'Sandeep', 'Anika', 'Amit', 'Neha', 'Raj', 'Smita', 'Vikram', 'Pooja']\n",
      "Added new country 'United States' with common names: ['John', 'Michael', 'Jennifer', 'Jessica', 'James', 'Robert', 'David', 'William', 'Mary', 'Linda']\n",
      "Added new country 'Vietnam' with common names: ['Minh', 'Hoa', 'An', 'Dung', 'Nga', 'Tuan', 'Lan', 'Phong', 'Thuy', 'Trung']\n"
     ]
    }
   ],
   "source": [
    "common_names_by_country = {}\n",
    "\n",
    "# Function to add new country and its common names to the dictionary\n",
    "def add_country_with_common_names(country_name, common_names):\n",
    "    if country_name in common_names_by_country:\n",
    "        print(f\"Country '{country_name}' already exists in the dictionary.\")\n",
    "    else:\n",
    "        common_names_by_country[country_name] = common_names\n",
    "        print(f\"Added new country '{country_name}' with common names: {common_names}\")\n",
    "\n",
    "# Add more countries and their common names\n",
    "add_country_with_common_names(\"Japan\", [\"Haruto\", \"Yui\", \"Haruka\", \"Yuki\", \"Hiroto\", \"Sakura\", \"Sora\", \"Aoi\", \"Riku\", \"Miyu\"])\n",
    "add_country_with_common_names(\"Italy\", [\"Alessio\", \"Sofia\", \"Giuseppe\", \"Aria\", \"Matteo\", \"Giulia\", \"Marco\", \"Emma\", \"Lorenzo\", \"Aurora\"])\n",
    "add_country_with_common_names(\"Australia\", [\"Oliver\", \"Charlotte\", \"William\", \"Ava\", \"Noah\", \"Mia\", \"Jack\", \"Amelia\", \"Leo\", \"Isla\"])\n",
    "add_country_with_common_names(\"France\", [\"Lucas\", \"Louise\", \"Hugo\", \"Manon\", \"Gabriel\", \"Chloé\", \"Raphaël\", \"Emma\", \"Jules\", \"Léa\"])\n",
    "add_country_with_common_names(\"South Korea\", [\"Min-Joon\", \"Ji-Yeon\", \"Hyeon-Woo\", \"Seo-Yeon\", \"Dae-Hyun\", \"Ha-Neul\", \"Sang-Min\", \"Su-Ji\", \"Hyun-Jae\", \"Yu-Jin\"])\n",
    "add_country_with_common_names(\"Mexico\", [\"Carlos\", \"Sofía\", \"Alejandro\", \"Valentina\", \"Javier\", \"Camila\", \"Miguel\", \"Andrea\", \"Diego\", \"Isabella\"])\n",
    "add_country_with_common_names(\"Canada\", [\"Ethan\", \"Olivia\", \"Liam\", \"Emma\", \"Noah\", \"Sophia\", \"Lucas\", \"Ava\", \"Benjamin\", \"Amelia\"])\n",
    "add_country_with_common_names(\"Egypt\", [\"Ahmed\", \"Nour\", \"Omar\", \"Hana\", \"Mohamed\", \"Sara\", \"Ali\", \"Yasmin\", \"Khaled\", \"Mariam\"])\n",
    "add_country_with_common_names(\"Nigeria\", [\"Chinedu\", \"Adebisi\", \"Obinna\", \"Chidinma\", \"Olufemi\", \"Chiamaka\", \"Emeka\", \"Adesua\", \"Uche\", \"Ngozi\"])\n",
    "add_country_with_common_names(\"Thailand\", [\"Narin\", \"Saranya\", \"Phirun\", \"Kanya\", \"Tawin\", \"Arunee\", \"Nawin\", \"Bundit\", \"Sopa\", \"Chai\"])\n",
    "add_country_with_common_names(\"Spain\", [\"Alejandro\", \"Sofía\", \"David\", \"Marta\", \"Pablo\", \"Lucía\", \"Carlos\", \"Laura\", \"Javier\", \"Alba\"])\n",
    "add_country_with_common_names(\"Greece\", [\"Giannis\", \"Maria\", \"Nikos\", \"Eleni\", \"Dimitris\", \"Katerina\", \"Yannis\", \"Despina\", \"Panagiotis\", \"Anastasia\"])\n",
    "add_country_with_common_names(\"Argentina\", [\"Mateo\", \"Sofía\", \"Valentín\", \"Camila\", \"Benjamín\", \"Valentina\", \"Santiago\", \"Emma\", \"Lucas\", \"Mia\"])\n",
    "add_country_with_common_names(\"Sweden\", [\"Elias\", \"Astrid\", \"Oscar\", \"Emilia\", \"Liam\", \"Alice\", \"Noah\", \"Olivia\", \"Viktor\", \"Ella\"])\n",
    "add_country_with_common_names(\"Switzerland\", [\"Liam\", \"Emma\", \"Noah\", \"Mia\", \"Elias\", \"Sofia\", \"Lukas\", \"Lina\", \"Gabriel\", \"Laura\"])\n",
    "add_country_with_common_names(\"South Africa\", [\"Lwazi\", \"Nompumelelo\", \"Kagiso\", \"Lerato\", \"Thabo\", \"Buhle\", \"Sipho\", \"Nokuthula\", \"Thabiso\", \"Nomvula\"])\n",
    "add_country_with_common_names(\"Indonesia\", [\"Rizky\", \"Ayu\", \"Wahyu\", \"Rani\", \"Adi\", \"Sari\", \"Budi\", \"Fitri\", \"Dian\", \"Hadi\"])\n",
    "add_country_with_common_names(\"Philippines\", [\"Juan\", \"Maria\", \"Javier\", \"Ana\", \"Gabriel\", \"Sophia\", \"Emmanuel\", \"Luisa\", \"Miguel\", \"Isabella\"])\n",
    "add_country_with_common_names(\"Poland\", [\"Jakub\", \"Zuzanna\", \"Szymon\", \"Julia\", \"Filip\", \"Maja\", \"Kacper\", \"Lena\", \"Bartosz\", \"Oliwia\"])\n",
    "add_country_with_common_names(\"Iran\", [\"Amir\", \"Sara\", \"Mohammad\", \"Yasmin\", \"Ali\", \"Hoda\", \"Hassan\", \"Mahsa\", \"Saeed\", \"Leila\"])\n",
    "add_country_with_common_names(\"Germany\", [\"Lukas\", \"Hannah\", \"Tim\", \"Anna\", \"Felix\", \"Lea\", \"Jan\", \"Laura\", \"Maximilian\", \"Sophie\"])\n",
    "add_country_with_common_names(\"Brazil\", [\"Carlos\", \"Ana\", \"Pedro\", \"Camila\", \"Lucas\", \"Isabella\", \"Mateus\", \"Juliana\", \"Rafael\", \"Laura\"])\n",
    "add_country_with_common_names(\"Russia\", [\"Ivan\", \"Olga\", \"Vladimir\", \"Anastasia\", \"Dmitry\", \"Svetlana\", \"Nikolai\", \"Tatiana\", \"Roman\", \"Maria\"])\n",
    "add_country_with_common_names(\"China\", [\"Wei\", \"Ling\", \"Ming\", \"Li\", \"Yan\", \"Chen\", \"Jing\", \"Tao\", \"Mei\", \"Jun\"])\n",
    "add_country_with_common_names(\"India\", [\"Ravi\", \"Priya\", \"Sandeep\", \"Anika\", \"Amit\", \"Neha\", \"Raj\", \"Smita\", \"Vikram\", \"Pooja\"])\n",
    "add_country_with_common_names(\"United States\", [\"John\", \"Michael\", \"Jennifer\", \"Jessica\", \"James\", \"Robert\", \"David\", \"William\", \"Mary\", \"Linda\"])\n",
    "add_country_with_common_names(\"Vietnam\", [\"Minh\", \"Hoa\", \"An\", \"Dung\", \"Nga\", \"Tuan\", \"Lan\", \"Phong\", \"Thuy\", \"Trung\"])\n",
    "# Function to generate random sample data and save to CSV\n",
    "def generate_and_save_sample(num_rows, csv_file):\n",
    "    # Get a list of country names\n",
    "    countries = [country.name for country in list(pycountry.countries)]\n",
    "\n",
    "    data = {\n",
    "        \"ID\": [i for i in range(num_rows)],\n",
    "        \"Name\": [f\"Person_{i}\" for i in range(num_rows)],\n",
    "        \"Age\": [random.randint(18, 60) for _ in range(num_rows)],\n",
    "        \"Salary\": [random.randint(30000, 100000) for _ in range(num_rows)],\n",
    "        \"Country\": [random.choice(countries) for _ in range(num_rows)]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Generate and save 1000-sample data\n",
    "generate_and_save_sample(1000, \"/data/sample_data_1000.csv\")\n",
    "\n",
    "# Generate and save 10000-sample data\n",
    "generate_and_save_sample(10000, \"/data/sample_data_10000.csv\")\n",
    "\n",
    "# Generate and save 100000-sample data\n",
    "generate_and_save_sample(100000, \"/data/sample_data_100000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfea2165-bebb-46f6-a59c-4a5f84810c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apot/.local/lib/python3.11/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/01 07:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: delta\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelta_paths[i]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Load data from Delta Lake table\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m df_delta, delta_load_time \u001b[38;5;241m=\u001b[39m \u001b[43mread_delta_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m df_delta\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Load data from CSV\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mread_delta_data\u001b[0;34m(delta_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_delta_data\u001b[39m(delta_path):\n\u001b[1;32m     16\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 17\u001b[0m     df_delta \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_delta, end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:738)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake vs CSV Performance Comparison\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define paths for Delta Lake and CSV files\n",
    "delta_paths = [\"/data/sample_data_1000_delta\", \"/data/sample_data_10000_delta\", \"/data/sample_data_100000_delta\"]\n",
    "csv_files = [\"/data/sample_data_1000.csv\", \"/data/sample_data_10000.csv\", \"/data/sample_data_100000.csv\"]\n",
    "\n",
    "# Function to read data from Delta Lake table and measure time\n",
    "def read_delta_data(delta_path):\n",
    "    start_time = time.time()\n",
    "    df_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "    end_time = time.time()\n",
    "    return df_delta, end_time - start_time\n",
    "\n",
    "# Function to read data from CSV and measure time\n",
    "def read_csv_data(csv_file):\n",
    "    start_time = time.time()\n",
    "    df_csv = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    end_time = time.time()\n",
    "    return df_csv, end_time - start_time\n",
    "\n",
    "# Dictionary to store performance results\n",
    "performance_results = {}\n",
    "\n",
    "# Perform load and measure time for Delta Lake and CSV for each sample size\n",
    "for i in range(len(delta_paths)):\n",
    "    print(f\"Sample Size: {delta_paths[i].split('_')[-1]}\")\n",
    "    \n",
    "    # Load data from Delta Lake table\n",
    "    df_delta, delta_load_time = read_delta_data(delta_paths[i])\n",
    "    df_delta.show()\n",
    "\n",
    "    # Load data from CSV\n",
    "    df_csv, csv_load_time = read_csv_data(csv_files[i])\n",
    "    df_csv.show()\n",
    "    \n",
    "    # Store performance results in the dictionary\n",
    "    performance_results[delta_paths[i].split('_')[-1]] = {\n",
    "        \"Delta Lake Load Time\": delta_load_time,\n",
    "        \"CSV Load Time\": csv_load_time\n",
    "    }\n",
    "    print(\"Delta Lake Load Time: {:.4f} seconds\".format(delta_load_time))\n",
    "    print(\"CSV Load Time: {:.4f} seconds\".format(csv_load_time))\n",
    "    print(\"--------------\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "# Create a chart to visualize the performance results\n",
    "sample_sizes = list(performance_results.keys())\n",
    "delta_load_times = [performance_results[size][\"Delta Lake Load Time\"] for size in sample_sizes]\n",
    "csv_load_times = [performance_results[size][\"CSV Load Time\"] for size in sample_sizes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sample_sizes, delta_load_times, label='Delta Lake')\n",
    "plt.bar(sample_sizes, csv_load_times, bottom=delta_load_times, label='CSV')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Load Time (seconds)')\n",
    "plt.title('Delta Lake vs CSV Performance Comparison')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702aa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake vs CSV Performance and Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define paths for Delta Lake and CSV files\n",
    "delta_paths = [\"/data/sample_data_1000_delta\", \"/data/sample_data_10000_delta\", \"/data/sample_data_100000_delta\"]\n",
    "csv_files = [\"/data/sample_data_1000.csv\", \"/data/sample_data_10000.csv\", \"/data/sample_data_100000.csv\"]\n",
    "\n",
    "# Function to read data from Delta Lake table and measure time\n",
    "def read_delta_data(delta_path):\n",
    "    start_time = time.time()\n",
    "    df_delta = spark.read.format(\"delta\").load(delta_path)\n",
    "    end_time = time.time()\n",
    "    return df_delta, end_time - start_time\n",
    "\n",
    "# Function to read data from CSV and measure time\n",
    "def read_csv_data(csv_file):\n",
    "    start_time = time.time()\n",
    "    df_csv = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
    "    end_time = time.time()\n",
    "    return df_csv, end_time - start_time\n",
    "\n",
    "# Function to perform data analysis\n",
    "def analyze_data(df):\n",
    "    # Most common name\n",
    "    most_common_name = df.groupBy(\"Name\").count().orderBy(\"count\", ascending=False).first().Name\n",
    "\n",
    "    # Country with the highest salary\n",
    "    country_highest_salary = df.groupBy(\"Country\").agg({\"Salary\": \"max\"}).orderBy(\"max(Salary)\", ascending=False).first().Country\n",
    "\n",
    "    # Relationship between salary and age, country\n",
    "    salary_age_correlation = df.corr(\"Salary\", \"Age\")\n",
    "    salary_country_correlation = df.groupBy(\"Country\").agg({\"Salary\": \"mean\"}).withColumnRenamed(\"avg(Salary)\", \"MeanSalary\").join(df, \"Country\").corr(\"MeanSalary\", \"Salary\")\n",
    "\n",
    "    return most_common_name, country_highest_salary, salary_age_correlation, salary_country_correlation\n",
    "\n",
    "# Dictionary to store performance results and analysis results\n",
    "results = {}\n",
    "\n",
    "# Perform load and analysis for Delta Lake and CSV for each sample size\n",
    "for i in range(len(delta_paths)):\n",
    "    print(f\"Sample Size: {delta_paths[i].split('_')[-1]}\")\n",
    "    \n",
    "    # Load data from Delta Lake table\n",
    "    df_delta, delta_load_time = read_delta_data(delta_paths[i])\n",
    "    \n",
    "    # Perform data analysis for Delta Lake\n",
    "    most_common_name_delta, country_highest_salary_delta, salary_age_correlation_delta, salary_country_correlation_delta = analyze_data(df_delta)\n",
    "\n",
    "    # Load data from CSV\n",
    "    df_csv, csv_load_time = read_csv_data(csv_files[i])\n",
    "\n",
    "    # Perform data analysis for CSV\n",
    "    most_common_name_csv, country_highest_salary_csv, salary_age_correlation_csv, salary_country_correlation_csv = analyze_data(df_csv)\n",
    "\n",
    "    # Store performance and analysis results in the dictionary\n",
    "    results[delta_paths[i].split('_')[-1]] = {\n",
    "        \"Delta Lake Load Time\": delta_load_time,\n",
    "        \"CSV Load Time\": csv_load_time,\n",
    "        \"Most Common Name (Delta Lake)\": most_common_name_delta,\n",
    "        \"Most Common Name (CSV)\": most_common_name_csv,\n",
    "        \"Country with Highest Salary (Delta Lake)\": country_highest_salary_delta,\n",
    "        \"Country with Highest Salary (CSV)\": country_highest_salary_csv,\n",
    "        \"Salary-Age Correlation (Delta Lake)\": salary_age_correlation_delta,\n",
    "        \"Salary-Age Correlation (CSV)\": salary_age_correlation_csv,\n",
    "        \"Salary-Country Correlation (Delta Lake)\": salary_country_correlation_delta,\n",
    "        \"Salary-Country Correlation (CSV)\": salary_country_correlation_csv\n",
    "    }\n",
    "    print(\"Delta Lake Load Time: {:.4f} seconds\".format(delta_load_time))\n",
    "    print(\"CSV Load Time: {:.4f} seconds\".format(csv_load_time))\n",
    "    print(\"--------------\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "# Print the analysis results\n",
    "for sample_size, analysis in results.items():\n",
    "    print(f\"Sample Size: {sample_size}\")\n",
    "    print(f\"Most Common Name (Delta Lake): {analysis['Most Common Name (Delta Lake)']}\")\n",
    "    print(f\"Most Common Name (CSV): {analysis['Most Common Name (CSV)']}\")\n",
    "    print(f\"Country with Highest Salary (Delta Lake): {analysis['Country with Highest Salary (Delta Lake)']}\")\n",
    "    print(f\"Country with Highest Salary (CSV): {analysis['Country with Highest Salary (CSV)']}\")\n",
    "    print(f\"Salary-Age Correlation (Delta Lake): {analysis['Salary-Age Correlation (Delta Lake)']:.4f}\")\n",
    "    print(f\"Salary-Age Correlation (CSV): {analysis['Salary-Age Correlation (CSV)']:.4f}\")\n",
    "    print(f\"Salary-Country Correlation (Delta Lake): {analysis['Salary-Country Correlation (Delta Lake)']:.4f}\")\n",
    "    print(f\"Salary-Country Correlation (CSV): {analysis['Salary-Country Correlation (CSV)']:.4f}\")\n",
    "    print(\"--------------\")\n",
    "\n",
    "# Create a chart to visualize the performance results\n",
    "sample_sizes = list(results.keys())\n",
    "delta_load_times = [results[size][\"Delta Lake Load Time\"] for size in sample_sizes]\n",
    "csv_load_times = [results[size][\"CSV Load Time\"] for size in sample_sizes]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(sample_sizes, delta_load_times, label='Delta Lake')\n",
    "plt.bar(sample_sizes, csv_load_times, bottom=delta_load_times, label='CSV')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Load Time (seconds)')\n",
    "plt.title('Delta Lake vs CSV Performance Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
